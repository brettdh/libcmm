A network is "troubled" when a TCP socket on that network stops making forward progress for too long.
A TCP socket has stopped making forward progress when unACKed data remains in the socket buffer.
-OR-
A TCP socket has stopped making forward progress when the window remains full; i.e. the kernel stops sending more data because it's waiting for the last window to be ACKed.
-OR-
A TCP socket has stopped making forward progress when there is unACKed data sent and no ACKs arriving.

From the TCP socket, I can get this information:
  - The amount of data in the send buffer
  - The timestamp of the last data sent
  - The timestamp of the last ack received
  - The smoothed RTT estimate
  - The RTT variance
  - The number of unACKed packets

Problems with socket-buffer-based approach:
  - Bytes in socket buffer may not have been sent yet

Problems with the window-based approach:
  - If the app isn't sending at least a full window, I won't detect a stall
  - Even if I know the number of bytes in flight hasn't changed, I don't know
    whether those are the same bytes that were in flight the last time I checked.
  - Could check whether the next-seqno-to-be-acked is increasing, but
    kernel doesn't expose that value, except indirectly through the
    number of bytes in the socket buffer (SIOCOUTQ ioctl).

So, I need to check whether there are any ACKs arriving at all, using the
last-ack-received timestamp.

First pass:
  ack_timeout = 2 * max(DELAYED_ACK_TIMEOUT, RTT)
  if ( there is at least one unacked packet; and
       the time since the last ack received is greater than ack_timeout) {
      declare network troubled
  }

The right timeout threshold is the tricky bit.  Too high, and we'll
stall communication; too low, and we'll retransmit wastefully.
Another approach could incorporate the RTT variance, which might give
me a probabalistic bound on the chance of spurious retransmission.
For steady-state bulk TCP transfer, an ACK should arrive every RTT
unless delayed ACKing is going on (i.e. many small uni-directional
transmissions).  So, waiting 2x the max of those delays should avoid
spurious retransmissions while still starting redundant retransmission
in under a second.  Incorporating the variance might allow a tighter
bound.

Given any timeout and method for declaring a network troubled,
the IntNW sender threads will check the TCP state at the earliest
expected timeout expiration.  There will be data structures keeping
track of which IROBs were sent on which networks, and if a network is
declared troubled, the sender thread will signal another sender thread
(if another network is available) to retransmit any missing data.
It does this by using the existing mechanism, which asks the receiver
to request retransmission of any missing IROB data or metadata.
It could inform the remote side that the other network is troubled,
but the remote sender could decide that independently.


Back-of-the-envelope characterization of this algorithm
---

Looking over the Ann Arbor and Ypsi traces from the IntNW paper, 
the RTT for wifi often stays in the range of 25-70ms, though as the
client moves and the connection quality changes, the RTT can jump up 
into the hundreds and even over 1 second occasionally.

For the below, assume that the maximum delayed ACK timeout is 200ms
(defined in Linux kernel as TCP_DELACK_MAX, though the kernel also
max-bounds this by its own RTT estimate).

First, let's consider normal operation at a RTT of 50ms.  
    The desired result is that the algorithm would not mark the wifi
    as troubled during successful data transfer.  Assume that the TCP
    RTT estimates for a socket on the wifi are stable (this assumption
    should be tested, but the stable EWMA filter should help ensure
    this).  Even if the RTT transiently becomes as large as ~100ms
    (unlikely in normal operation), we will continue trying to use the
    network.  

    It's conceivable that the RTT might spike this much
    during normal operation, so I ran a simple experiment to check
    whether it does.  Using wifi on the Nexus One, I connected a
    socket to the speedtest server and uploaded bytes as fast as
    possible.  Every 50ms, I sampled the RTT and the time since the
    last ACK.  I ran the test for 30 seconds.  If a sample reported
    that the above algorithm would mark the network troubled, I
    recorded it.  Running this experiment a few times, I observe
    that the network is marked troubled in 0.0-0.6% of the ~600 samples.
    Though small, this number should really be zero, indicating that
    the algorithm might need to be more conservative.

    Nevertheless, it should be likely that an ACK timeout represents 
    a real network failure.  Finally, the ACK timeout is short enough
    to trigger in less than a second, ensuring quick recovery.

Next, let's consider normal operation on a 3G network, at an RTT of 200ms.



Missing:
  - How to remove a recovered network from trouble mode.

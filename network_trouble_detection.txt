A network is "troubled" when a TCP socket on that network stops making forward progress for too long.
A TCP socket has stopped making forward progress when unACKed data remains in the socket buffer.
-OR-
A TCP socket has stopped making forward progress when the window remains full; i.e. the kernel stops sending more data because it's waiting for the last window to be ACKed.
-OR-
A TCP socket has stopped making forward progress when there is unACKed data sent and no ACKs arriving.

From the TCP socket, I can get this information:
  - The amount of data in the send buffer
  - The timestamp of the last data sent
  - The timestamp of the last ack received
  - The smoothed RTT estimate
  - The RTT variance
  - The number of unACKed packets

Problems with socket-buffer-based approach:
  - Bytes in socket buffer may not have been sent yet

Problems with the window-based approach:
  - If the app isn't sending at least a full window, I won't detect a stall
  - Even if I know the number of bytes in flight hasn't changed, I don't know
    whether those are the same bytes that were in flight the last time I checked.
  - Could check whether the next-seqno-to-be-acked is increasing, but
    kernel doesn't expose that value, except indirectly through the
    number of bytes in the socket buffer (SIOCOUTQ ioctl).

So, I need to check whether there are any ACKs arriving at all, using the
last-ack-received timestamp.

First pass:
  ack_timeout = 2 * max(DELAYED_ACK_TIMEOUT, RTT)
  if ( there is at least one unacked packet; and
       the time since the last ack received is greater than ack_timeout) {
      declare network troubled
  }

The right timeout threshold is the tricky bit.  Too high, and we'll
stall communication; too low, and we'll retransmit wastefully.
Another approach could incorporate the RTT variance, which might give
me a probabalistic bound on the chance of spurious retransmission.
For steady-state bulk TCP transfer, an ACK should arrive every RTT
unless delayed ACKing is going on (i.e. many small uni-directional
transmissions).  So, waiting 2x the max of those delays should avoid
spurious retransmissions while still starting redundant retransmission
in under a second.  Incorporating the variance might allow a tighter
bound.

Given any timeout and method for declaring a network troubled,
the IntNW sender threads will check the TCP state at the earliest
expected timeout expiration.  There will be data structures keeping
track of which IROBs were sent on which networks, and if a network is
declared troubled, the sender thread will signal another sender thread
(if another network is available) to retransmit any missing data.
It does this by using the existing mechanism, which asks the receiver
to request retransmission of any missing IROB data or metadata.
It could inform the remote side that the other network is troubled,
but the remote sender could decide that independently.


Back-of-the-envelope characterization of this algorithm
---

Looking over the Ann Arbor and Ypsi traces from the IntNW paper, 
the RTT for wifi often stays in the range of 25-70ms, though as the
client moves and the connection quality changes, the RTT can jump up 
into the hundreds and even over 1 second occasionally.

For the below, assume that the maximum delayed ACK timeout is 200ms
(defined in Linux kernel as TCP_DELACK_MAX, though the kernel also
max-bounds this by its own RTT estimate).

First, let's consider normal operation at a RTT of 50ms.  
    The desired result is that the algorithm would not mark the wifi
    as troubled during successful data transfer.  Assume that the TCP
    RTT estimates for a socket on the wifi are stable (this assumption
    should be tested, but the stable EWMA filter should help ensure
    this).  Even if the RTT transiently becomes as large as ~100ms
    (unlikely in normal operation), we will continue trying to use the
    network.  

    It's conceivable that the RTT might spike this much
    during normal operation, so I ran a simple experiment to check
    whether it does.  Using wifi on the Nexus One, I connected a
    socket to the speedtest server and uploaded bytes as fast as
    possible.  Every 50ms, I sampled the RTT and the time since the
    last ACK.  I ran the test for 30 seconds.  If a sample reported
    that the above algorithm would mark the network troubled, I
    recorded it.  Running this experiment a few times, I observe
    that the network is marked troubled in 0.0-0.6% of the ~600 samples.
    Though small, this number should really be zero, indicating that
    the algorithm might need to be more conservative.

    Nevertheless, it should be likely that an ACK timeout represents 
    a real network failure.  Finally, the ACK timeout is short enough
    to trigger in less than a second, ensuring quick recovery.

Next, let's consider normal operation on a 3G network, at an RTT of 200ms.
    Desired result is the same as above.  The upper bound on transient
    RTT spikes is 400ms.  For this experiment, the variance is
    enormous. In many runs, 2-3% of the ~600 samples are marked as
    troubled, but I also see runs in which zero samples are in
    trouble, runs with 6-12% trouble, and runs in which 45% (!) of
    samples are in trouble.  I also notice that the smoothed RTT estimate
    reported by the kernel swells to over 2-3 seconds during the test
    runs.  It looks like the runs with large trouble percentages are
    due to the RTT estimate being lower even though there are
    sometimes significant ACK delays.

    Then again, since we'll only be running trouble-checks on FG
    traffic, and since 3G is rarely the FG network, perhaps this won't
    really come up much in practice.

Next, let's consider operation on wifi with a link failure; i.e. 
how quickly does the algorithm recognize the failure?
    To test this, I ran the above experiment and turned off the wifi
    radio in the middle.  I kept the sample period of 50ms and printed
    out a message at the first sample deemed to be in trouble.  The
    reaction is practically immediate, as expected, since the time
    since the last ack increases much more rapidly than the relatively
    stable TCP RTT estimate.

    To test the recovery of a network from trouble mode back to normal
    operation, I did the same experiment, but I turned the wifi radio
    back on after a couple seconds.  The recovery is a little slower,
    since it depends on the TCP retransmission timeout, which
    increases exponentially in the backoff period.  Since the total
    RTO growth is proportional to the length of the transient
    connection interruption, the interruptions that are more likely to
    be real trouble are also more likely to be marked as trouble by
    the algorithm, at which point the recovery matters less, since the
    network is probably going away.

Finally, let's look at what happens with natural wifi fading.
    To test this, I ran the experiment while walking away from the
    AP.  As expected, there's a point after which the signal strength
    is so low that the TCP connection isn't able to make any
    progress.  The interesting thing is that this occurs many seconds
    before the Android system decides that the wifi is gone, making
    for much faster recovery.

    As above, the reactivity is biased towards quickly deciding that
    the wifi is in trouble and more slowly deciding that it has
    recovered, due again to the quickly increasing RTO.  Near the
    boundary of the AP's coverage, then, there's a critical point at
    which the algorithm will report trouble.


This approach, then, though simple, looks like it will significantly
improve the usage of wifi over the current approach of waiting for an
Android notification before retransmitting potentially lost data.
Further, in normal wifi usage, it seems unlikely that false positives
in the trouble detection will have a significant impact.  However, to
be a touch more conservative, we could impose a floor of 200ms on the
ACK timeout, similar to how the Linux kernel lower-bounds the TCP RTO
at 200ms.  This would get rid of all the spurious trouble markings
that I've seen in my working wifi experiments while still being very
responsive to true failures.

At this point, I think it's reasonable to move on to adding this
detection to IntNW and testing it out.
